{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ba4d634-81aa-4542-b613-5cd69885bf6f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark Session & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76881c7e-ee69-4b2f-988a-0f85cc830384",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize pyspark session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"12g\")\n",
    "    .appName(\"Partitioning\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb1f9625-d2d8-44b5-a80c-21a7a6e5c55a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load and cache data from \"features.csv\"\n",
    "df = spark.read.csv(\"features.csv\", header=True, inferSchema=True)\n",
    "df.cache()\n",
    "\n",
    "# Reorder the columns in the DataFrame such that the \"outcome\" column is the last one\n",
    "columns = df.columns\n",
    "columns_reordered = [col_name for col_name in columns if col_name != \"outcome\"] + [\n",
    "    \"outcome\"\n",
    "]\n",
    "df = df.select(*columns_reordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "425c9e3b-821c-4a1e-9eec-907f9b47f185",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af93706b-6e36-4652-ab14-21c39350f548",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a9dc020-465d-4cdd-9f17-f0903a470259",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Time analysis functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7947b527-e649-41e7-849c-2cebee5f3c64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Time analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe7074b4-bfee-4386-bf59-b56af511e157",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "def time_analysis(df: DataFrame, func) -> dict:\n",
    "    \"\"\"\n",
    "    Apply the provided function to different fractions (subsets) of the DataFrame and measure the time taken\n",
    "    for each function run.\n",
    "\n",
    "    Parameters:\n",
    "    df : DataFrame\n",
    "        The DataFrame to apply the function to.\n",
    "    func : function\n",
    "        The function to apply to the DataFrame. The function should take a DataFrame as its only argument.\n",
    "\n",
    "    Returns:\n",
    "    dict\n",
    "        A dictionary containing the fraction of DataFrame and the corresponding time taken for the function to run.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    df_length = df.count()\n",
    "\n",
    "    for percentage in range(10, 110, 10):\n",
    "        # Calculate the number of rows for the given percentage\n",
    "        num_rows = int(df_length * (percentage / 100))\n",
    "        partial_df = df.limit(num_rows)\n",
    "\n",
    "        # Measure the time taken to apply the function\n",
    "        start_time = time()\n",
    "        func(partial_df)\n",
    "        end_time = time()\n",
    "\n",
    "        # Store the results\n",
    "        results[float(percentage / 100)] = end_time - start_time\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b80b2d3-eb1f-4ccd-a3f1-60279aec50df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Plot time analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21606f61-dbcf-46c6-8313-f7e4d4694647",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "\n",
    "def plot_time_analysis(times_partitioning: dict, plot_title: str):\n",
    "    \"\"\"\n",
    "    Plot the time taken for the function to run for different fractions of the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    times_partitioning : dict\n",
    "        A dictionary containing the fraction of DataFrame and the corresponding time taken for the function to run.\n",
    "    plot_title : str\n",
    "        The title of the plot.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.plot(list(times_partitioning.keys()), list(times_partitioning.values()))\n",
    "\n",
    "    plt.xlabel(\"Fraction of DataFrame\")\n",
    "\n",
    "    plt.ylabel(\"Time taken (s)\")\n",
    "    # plt.title(plot_title)\n",
    "\n",
    "    plot_filename = \"\".join(e for e in plot_title if e.isalnum() or e == \"_\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"{plot_filename}.pdf\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15816b26-b55b-4d98-ba55-4962bea6ea1e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44bf3833-d383-41bd-9478-58bedc509691",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We'll just use line number for the bidder id\n",
    "df = df.drop(\"bidder_id\", \"payment_account\", \"address\").withColumnRenamed(\n",
    "    \"_c0\", \"bidder_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d55cf911-d7b9-4eed-80d9-39592b484761",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop rows will nulls\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d87db61c-0d03-4884-a4bb-30fe5294b9e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30030586-ae13-47f5-abeb-e449a9f6fb8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def print_sample_counts(df: DataFrame):\n",
    "    \"\"\"\n",
    "    Print the number and percentage of samples in the DataFrame: Total; outcome = 1; outcome = 0.\n",
    "\n",
    "    Parameters:\n",
    "    df : DataFrame - The DataFrame to count samples from.\n",
    "    \"\"\"\n",
    "    # Number of samples in df\n",
    "    total_samples = df.count()\n",
    "    print(\"Total samples:\", total_samples)\n",
    "\n",
    "    # Number of samples where outcome = 1\n",
    "    outcome_1_samples = df.filter(df.outcome == 1).count()\n",
    "    print(\"Samples where outcome = 1:\", outcome_1_samples)\n",
    "\n",
    "    # Number of samples where outcome = 0\n",
    "    outcome_0_samples = df.filter(df.outcome == 0).count()\n",
    "    print(\"Samples where outcome = 0:\", outcome_0_samples)\n",
    "\n",
    "    # Percentage of samples where outcome = 1\n",
    "    outcome_1_percentage = (outcome_1_samples / total_samples) * 100\n",
    "    print(f\"Percentage of samples where outcome = 1: {outcome_1_percentage:.2f}%\")\n",
    "\n",
    "    # Print the number of partitions of df\n",
    "    num_partitions = df.rdd.getNumPartitions()\n",
    "    print(\"Number of partitions:\", num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67e19a55-bec3-4503-9609-da4e27aea7d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print_sample_counts(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06fa86ed-b169-4190-930f-3178bb489f81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Oversampling & Test/Train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a64ad7ac-ced5-4b48-9bac-c9b9d2847141",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform a train-test split\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=21)\n",
    "\n",
    "\"\"\"\n",
    "Over-sampling the minority class (outcome = 1) in the training set by a factor of 4.\n",
    "This will help to balance the classes and improve the model's performance.\n",
    "\"\"\"\n",
    "outcome_1_samples = train_df.filter(train_df.outcome == 1)\n",
    "train_df = train_df.union(outcome_1_samples.sample(True, 4.0, seed=42))\n",
    "\n",
    "print_sample_counts(train_df)\n",
    "print_sample_counts(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fee03ea2-4113-469d-98d6-65891b9c3603",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "177cfaa9-2f56-481c-9dd7-9340ad64f64c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Partitioning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2d64c88-bcc5-4c8d-906d-3f2a9d6ba65e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def rf_partition_dataframe(\n",
    "    df: DataFrame,\n",
    "    num_partitions: int = 10,\n",
    "    partition_size: float = 0.8,\n",
    "    seed: int = 2137,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Randomly sample num_partitions of samples from the original dataframe for each partition. The resulting dataframe will be\n",
    "    (num_partitions * partition_size) times as large as the original dataframe, divided into num_partitions partitions.\n",
    "    This is intended for use with Random Forests utilizing local approach.\n",
    "\n",
    "    Parameters:\n",
    "        df : DataFrame\n",
    "            The original dataframe.\n",
    "        num_partitions : int, optional\n",
    "            The number of partitions to create. Default is 10.\n",
    "        partition_size : float, optional\n",
    "            The size of each partition as a fraction of the original dataframe. Default is 0.8.\n",
    "        seed : int, optional\n",
    "            The seed for the random number generator. Default is 2137. After function execution, the random number generator\n",
    "            will be restored to its previous state.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "            The resulting dataframe with (num_partitions * partition_size) times as large as the original dataframe,\n",
    "            divided into num_partitions partitions.\n",
    "\n",
    "    Raises:\n",
    "        ValueError\n",
    "            If num_partitions is less than 1 or partition_size is not in the range (0, 1].\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if \"num_partitions\" is valid\n",
    "    if num_partitions < 1:\n",
    "        raise ValueError(\n",
    "            \"rf_partition_dataframe: num_partitions must be greater than or equal to 1.\"\n",
    "        )\n",
    "\n",
    "    # Check if \"partition_size\" is valid\n",
    "    if partition_size <= 0 or partition_size > 1:\n",
    "        raise ValueError(\n",
    "            \"rf_partition_dataframe: partition_size must be in the range (0, 1].\"\n",
    "        )\n",
    "\n",
    "    # Save the current state of the random number generator and set the seed\n",
    "\n",
    "    rng_state = random.getstate()\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Create dataframe with data for initial partition\n",
    "\n",
    "    partitioned_df = df.sample(\n",
    "        withReplacement=False, fraction=partition_size, seed=random.randint(100, 1000)\n",
    "    )\n",
    "\n",
    "    partitioned_df = partitioned_df.withColumn(\"partition\", F.lit(0))\n",
    "\n",
    "    # Create additional partitions\n",
    "\n",
    "    for partition_num in range(1, num_partitions):\n",
    "        # Randomly sample \"parition_size\" of samples from each partition\n",
    "        partition_df = df.sample(\n",
    "            withReplacement=False,\n",
    "            fraction=partition_size,\n",
    "            seed=random.randint(100, 1000),\n",
    "        )\n",
    "\n",
    "        # Add a column to the sampled_df to indicate the partition it came from\n",
    "        partition_df = partition_df.withColumn(\"partition\", F.lit(partition_num))\n",
    "\n",
    "        # Add partition_df to sampled_df\n",
    "        partitioned_df = partitioned_df.union(partition_df)\n",
    "\n",
    "    # Perform partitioning based on column \"partition\"\n",
    "    partitioned_df = partitioned_df.repartitionByRange(\n",
    "        num_partitions + 1, \"partition\"\n",
    "    )  # No clue why it does need to be num_partitions + 1.\n",
    "    # If I don't add 1, it shows correct (target) number of partitions but one of them holds two \"virtual\" partitions.\n",
    "\n",
    "    # Restore the state of the random number generator\n",
    "\n",
    "    random.setstate(rng_state)\n",
    "\n",
    "    return partitioned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33ac4f99-0021-4956-9623-5de4a85fd153",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Check correctness of partitioning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72e3446d-f45e-4cc5-8384-71d8284f7bac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check correctness of partitioning\n",
    "\n",
    "check_df = rf_partition_dataframe(train_df, num_partitions=6, partition_size=0.8)\n",
    "\n",
    "# Print number of partitions\n",
    "print(\"Number of partitions for check_df:\", check_df.rdd.getNumPartitions())\n",
    "\n",
    "# Check correctness of partitioning\n",
    "check_df = check_df.withColumn(\"actual_partition\", F.spark_partition_id())\n",
    "check_df.groupBy(\"actual_partition\", \"partition\").agg(\n",
    "    F.count(\"*\").alias(\"total_per_virtual_partition\")\n",
    ").show()\n",
    "\n",
    "# Number of elements in each actual partition\n",
    "check_df.groupBy(\"actual_partition\").agg(\n",
    "    F.count(\"*\").alias(\"total_per_partition\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f86a238-3056-418d-9b36-a906ff6e8412",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Time analysis - Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52b8e802-b950-458a-a417-a77ea6b5c2e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "times_partitioning = time_analysis(\n",
    "    train_df,\n",
    "    lambda df: rf_partition_dataframe(\n",
    "        df=df, num_partitions=10, partition_size=0.8, seed=2137\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "print(times_partitioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "192c1210-f81e-4599-ad39-8fae681edc98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_time_analysis(times_partitioning, \"Time Analysis of Partitioning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "890ce15c-1627-4578-96d9-f531626689a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Prepare RDDs for Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f37b3502-e1f9-4a7d-9df6-6b61e871210d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train (Partition data for local approach RF)\n",
    "print(\"Train:\")\n",
    "df_partitioned = rf_partition_dataframe(train_df, num_partitions=10).drop(\"partition\")\n",
    "rdd_train = df_partitioned.rdd.cache()\n",
    "print(rdd_train.getNumPartitions())\n",
    "print(rdd_train.glom().map(len).collect())\n",
    "\n",
    "# Test (no partitioning)\n",
    "print(\"Test:\")\n",
    "rdd_test = test_df.rdd.repartition(3).cache()\n",
    "print(rdd_test.getNumPartitions())\n",
    "print(rdd_test.glom().map(len).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f53cee3d-dc74-4093-8e28-aa8d7711fe07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Local approach Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "594294e1-40b8-4639-b6ed-c5bfb2177247",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Local model - build function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "525e39ec-d8c9-4465-84f2-cf00a78c9bf9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# We will use all columns except \"outcome\" as features, as feature selection revealed that all columns are important\n",
    "selected_features = df_partitioned.columns\n",
    "\n",
    "\n",
    "def build_local_tree(partition_iter, parameters: dict):\n",
    "    \"\"\"\n",
    "    Build a Decision Tree Classifier using the data in the partition_iter.\n",
    "\n",
    "    Parameters:\n",
    "        partition_iter : iterator\n",
    "            An iterator containing the data for training the Decision Tree Classifier.\n",
    "        parameters : dict\n",
    "            The hyperparameters for the Decision Tree Classifier.\n",
    "\n",
    "    Returns:\n",
    "        list\n",
    "            An iterable containing the trained Decision Tree Classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the partition_iter to a pandas DataFrame\n",
    "    pd_partition = pd.DataFrame(partition_iter, columns=selected_features)\n",
    "\n",
    "    # Check if the pd_partition is empty\n",
    "    if pd_partition.empty:\n",
    "        return []  # Skip training if the partition is empty\n",
    "\n",
    "    # Divide the dataframe into features (X_train) and target (y_train)\n",
    "    X_train = pd_partition.drop(\"outcome\", axis=1)\n",
    "    y_train = pd_partition[\"outcome\"]\n",
    "\n",
    "    # Create and train a Decision Tree Classifier\n",
    "    classifier = DecisionTreeClassifier(**parameters)\n",
    "\n",
    "    # Fit the classifier to the training data\n",
    "    model = classifier.fit(X_train.values, y_train)\n",
    "    return [model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "433de1f9-6299-4bbb-9fdb-9a630d0cd211",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af761f60-cc24-4d50-8951-25788476e1d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example hyperparameters for the Decision Tree Classifier\n",
    "example_parameters = {\n",
    "    \"criterion\": \"gini\",\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_split\": 2,\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"max_features\": None,\n",
    "    \"class_weight\": \"balanced\",\n",
    "}\n",
    "\n",
    "build_wrapper = lambda partition_iter: build_local_tree(\n",
    "    partition_iter, example_parameters\n",
    ")\n",
    "models = rdd_train.mapPartitions(build_wrapper).collect()\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7cc33b4-4088-413e-98b5-b59f674dd70b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "817b9d41-81ba-4e0a-987e-35711dc41b96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def predict(instance, models):\n",
    "    \"\"\"\n",
    "    Predict the outcome for a given instance using the trained models.\n",
    "    Requires a wrapper to be used with the rdd_test.map() function.\n",
    "\n",
    "    Parameters:\n",
    "        instance : list\n",
    "            A list containing the features of the instance.\n",
    "        models : list\n",
    "            A list containing the trained models.\n",
    "\n",
    "    Returns:\n",
    "        int\n",
    "            The predicted outcome for the instance.\n",
    "    \"\"\"\n",
    "    # Get input features\n",
    "    features = instance[:-1]\n",
    "\n",
    "    # Predict the outcome for the instance using all the trained models\n",
    "    predictions = [model.predict([features])[0] for model in models]\n",
    "\n",
    "    # Return the most common prediction\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7660c91c-98a7-4e0e-979a-d9440662c598",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def agg_predictions(predictions):\n",
    "    \"\"\"\n",
    "    Aggregate the predictions for a given instance.\n",
    "\n",
    "    Parameters:\n",
    "        predictions : list\n",
    "            A list containing the predictions for the instance.\n",
    "\n",
    "    Returns:\n",
    "        int\n",
    "            The aggregated prediction for the instance.\n",
    "    \"\"\"\n",
    "    # Count the number of 1s and 0s in the predictions\n",
    "    num_1 = predictions.count(1)\n",
    "    num_0 = predictions.count(0)\n",
    "\n",
    "    # Return the most common prediction\n",
    "    return 1.0 if num_1 > num_0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8170dc33-262d-4694-903f-3316bdf70a09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predict_wrapper = lambda instance: predict(instance, models)\n",
    "preds = rdd_test.map(predict_wrapper).collect()\n",
    "print(preds[:10])\n",
    "preds_agg = rdd_test.map(predict_wrapper).map(agg_predictions).collect()\n",
    "print(preds_agg[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36e35a26-4313-4c54-b698-d5598078454a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "def transform(instance, models):\n",
    "    \"\"\"\n",
    "    Transform the instance by adding the aggregated prediction to the end of the instance.\n",
    "    Requires a wrapper to be used with the rdd_test.map() function.\n",
    "\n",
    "    Parameters:\n",
    "        instance : list\n",
    "            A list containing the features of the instance.\n",
    "\n",
    "    Returns:\n",
    "        list\n",
    "            A list containing the features of the instance and the aggregated prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict the outcome for the instance\n",
    "    raw_prediction = agg_predictions(predict(instance, models))\n",
    "\n",
    "    # Return the instance with the aggregated prediction\n",
    "    return Row(**instance.asDict(), raw_prediction=raw_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca244da0-724f-4d89-aaa2-242457c64bfb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transform_wrapper = lambda instance: transform(instance, models)\n",
    "df_pred = rdd_test.map(transform_wrapper).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f618598-ebc4-4bdf-8e5f-1365eab61fc8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8073de2-9463-4999-8768-250077f73cc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "# BinaryClassificationEvaluator().explainParams()\n",
    "\n",
    "# Evaluator to calculate the area under the ROC curve\n",
    "evaluator_localRF = BinaryClassificationEvaluator(\n",
    "    labelCol=\"outcome\", rawPredictionCol=\"raw_prediction\", metricName=\"areaUnderROC\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6228f802-e1e2-4c5b-a77f-ce6497caf023",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "evaluator_localRF.evaluate(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af5f131f-b64c-4d85-be82-f8de150a64da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Parameter Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bceee926-fbb2-4c53-bc57-c325b994fefa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    \"splitter\": [\"best\"],  # \"random\" is the other option, it's generally worse\n",
    "    \"max_depth\": [\n",
    "        5,\n",
    "        10,\n",
    "        15,\n",
    "    ],  # Might need to increase, or try None. Will see when fitting.\n",
    "    \"min_samples_split\": [2, 4, 6],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\n",
    "        None,\n",
    "        \"sqrt\",\n",
    "        \"log2\",\n",
    "    ],  # None: all features, \"sqrt\": root of # features, \"log2\": log2 # features. Can also try float proportions.\n",
    "    \"class_weight\": [\n",
    "        None,\n",
    "        \"balanced\",\n",
    "    ],  # Balanced adjusts weights inverse prop. to class frequencies in input. Prob useful for us cos imbalance.\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Create a list of hyperparameter combinations\n",
    "hyperparameter_combinations = list(ParameterGrid(param_grid))\n",
    "\n",
    "print(\"Number of hyperparameter combinations:\", len(hyperparameter_combinations))\n",
    "print(hyperparameter_combinations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc251fe3-7d51-42d3-a5af-3ff559ad5d35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def parameter_search_CV(\n",
    "    df: DataFrame,\n",
    "    param_grid: list,\n",
    "    num_partitions: int = 10,\n",
    "    folds: int = 4,\n",
    "    verbose: bool = False,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Perform hyperparameter search using cross-validation on the given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df: DataFrame\n",
    "            The DataFrame to perform hyperparameter search on.\n",
    "        param_grid: list\n",
    "            A list containing dictionaries of hyperparameters.\n",
    "        num_partitions: int, optional\n",
    "            The number of partitions to create, this will also serve as number of trees in the Random Forest. Default is 10.\n",
    "        folds: int, optional\n",
    "            The number of folds to use in cross-validation. Default is 4.\n",
    "        verbose: bool, optional\n",
    "            If set to True print the results of each hyperparameter combination. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        tuple\n",
    "            A tuple containing the best hyperparameters dict and the corresponding mean AUC value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Best hyperparameters and corresponding AUC\n",
    "    best_hyperparameters = None\n",
    "    best_auc = -1.0\n",
    "\n",
    "    # Divide dataframes into k folds\n",
    "    folds_dfs = df.randomSplit([1 / folds] * folds, seed=37)\n",
    "\n",
    "    # Prepare train/validation splits\n",
    "    train_validation_splits = []\n",
    "\n",
    "    for fold in range(folds):\n",
    "        # Validation DataFrame\n",
    "        rdd_validation = folds_dfs[fold].rdd.repartition(2).cache()\n",
    "\n",
    "        # Train\n",
    "        train_dfs = [folds_dfs[i] for i in range(folds) if i != fold]\n",
    "        df_train = train_dfs[0]\n",
    "\n",
    "        for i in range(1, len(train_dfs)):\n",
    "            df_train = df_train.union(train_dfs[i])\n",
    "\n",
    "        # Partition the training datafram\n",
    "        train_partitioned = rf_partition_dataframe(\n",
    "            df_train, num_partitions=num_partitions, partition_size=0.9\n",
    "        ).drop(\"partition\")\n",
    "\n",
    "        rdd_train = train_partitioned.rdd.cache()\n",
    "\n",
    "        # Append the train/validation splits\n",
    "        train_validation_splits.append((rdd_train, rdd_validation))\n",
    "\n",
    "    # Perform hyperparameter search using cross-validation\n",
    "    for param_dict in tqdm(param_grid):\n",
    "        new_best = False\n",
    "\n",
    "        # Initialize list to store AUC values for each fold\n",
    "        auc_list = []\n",
    "\n",
    "        # Prepare build_wrapper with current hyperparameters\n",
    "        build_wrapper = lambda partition_iter: build_local_tree(\n",
    "            partition_iter, param_dict\n",
    "        )\n",
    "\n",
    "        # Start time\n",
    "        time_start = time()\n",
    "\n",
    "        # Perform hyperparameter search using cross-validation\n",
    "        for rdd_train, rdd_validation in train_validation_splits:\n",
    "            # Train the models\n",
    "            models = rdd_train.mapPartitions(build_wrapper).collect()\n",
    "\n",
    "            # Transform the test data (predictions)\n",
    "            transform_wrapper = lambda instance: transform(instance, models)\n",
    "            df_pred = rdd_validation.map(transform_wrapper).toDF()\n",
    "\n",
    "            # Evaluate the predictions\n",
    "            evaluator = BinaryClassificationEvaluator(\n",
    "                labelCol=\"outcome\",\n",
    "                rawPredictionCol=\"raw_prediction\",\n",
    "                metricName=\"areaUnderROC\",\n",
    "            )\n",
    "            auc = evaluator.evaluate(df_pred)\n",
    "\n",
    "            # Store the AUC value\n",
    "            auc_list.append(auc)\n",
    "\n",
    "        # Calculate the mean AUC value for the current hyperparameters\n",
    "        mean_auc = sum(auc_list) / len(auc_list)\n",
    "\n",
    "        # Update the best hyperparameters and AUC value if the current hyperparameters are better\n",
    "        if mean_auc > best_auc:\n",
    "            best_auc = mean_auc\n",
    "            best_hyperparameters = param_dict\n",
    "            new_best = True\n",
    "\n",
    "        # Print the results if verbose is enabled\n",
    "        if verbose:\n",
    "            print(f\"Hyperparameters: {param_dict}\")\n",
    "            print(f\"Mean AUC: {mean_auc}\")\n",
    "            print(f\"New best: {new_best}\")\n",
    "            print(f\"Time taken: {time() - time_start:.2f} seconds\")\n",
    "            print()\n",
    "\n",
    "    print(\"Number of hyperparameter combinations searched through:\", len(param_grid))\n",
    "    print(\"Best hyperparameters:\", best_hyperparameters)\n",
    "    print(\"Best mean AUC:\", best_auc)\n",
    "\n",
    "    return best_hyperparameters, best_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e8809db-ea26-4591-9874-ce4651b8b6cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find the best hyperparameters and corresponding AUC value\n",
    "# chosen_hyperparameters_combinations = random.sample(hyperparameter_combinations, 3)\n",
    "# best_params_localRF, best_auc_localRF = parameter_search_CV(\n",
    "#     train_df,\n",
    "#     chosen_hyperparameters_combinations,\n",
    "#     num_partitions=10,\n",
    "#     folds=4,\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "best_params_localRF, best_auc_localRF = parameter_search_CV(\n",
    "    train_df,\n",
    "    hyperparameter_combinations,\n",
    "    num_partitions=10,\n",
    "    folds=4,\n",
    "    verbose=False,\n",
    ")\n",
    "# Best hyperparameters: {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 15, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 4, 'splitter': 'best'}\n",
    "# Best mean AUC: 0.9880130419272364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e335e2d-50a2-4e7f-adef-1c07b16f0ab2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Best hyperparameters: {best_params_localRF}\")\n",
    "print(f\"Best mean AUC: {best_auc_localRF}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91d1bcc5-3f75-4c94-b29e-4be98903d500",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Time analysis - local approach RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9672d8c7-7d4e-418a-a595-1240336a8f26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def time_analysis_train_and_predict(train_df: DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Time the training and prediction of the Random Forest model for different fractions of the training DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    train_df : DataFrame\n",
    "        The training DataFrame to train the Random Forest model on.\n",
    "\n",
    "    Returns:\n",
    "    dict\n",
    "        A dictionary containing the fraction of DataFrame and the corresponding time taken for the training and prediction.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    df_length = train_df.count()\n",
    "\n",
    "    for percentage in range(10, 110, 10):\n",
    "        # Calculate the number of rows for the given percentage\n",
    "        num_rows = int(df_length * (percentage / 100))\n",
    "        partial_df = train_df.limit(num_rows)\n",
    "\n",
    "        # Prepare partitions for Random Forest (not taken into account for timing)\n",
    "        rdd_partial = (\n",
    "            rf_partition_dataframe(partial_df, num_partitions=10)\n",
    "            .drop(\"partition\")\n",
    "            .rdd.cache()\n",
    "        )\n",
    "\n",
    "        # Time train and prediction\n",
    "        start_time = time()\n",
    "\n",
    "        # Train\n",
    "        build_wrapper = lambda partition_iter: build_local_tree(\n",
    "            partition_iter, example_parameters\n",
    "        )\n",
    "        models = rdd_partial.mapPartitions(build_wrapper).collect()\n",
    "\n",
    "        # Predict\n",
    "        transform_wrapper = lambda instance: transform(instance, models)\n",
    "        df_pred = rdd_test.map(transform_wrapper).toDF()\n",
    "\n",
    "        end_time = time()\n",
    "\n",
    "        # Store the results\n",
    "        results[float(percentage / 100)] = end_time - start_time\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a57b9bd-a5ed-4889-9766-c259ec7b8ce8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "times_train_and_predict_localRF = time_analysis_train_and_predict(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9339a7b5-75c2-4e6d-a99e-da2008b37385",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_time_analysis(\n",
    "    times_train_and_predict_localRF,\n",
    "    \"Time Analysis of Training and Prediction with Local approach Random Forest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f805782d-7649-489f-b011-8757fdbd5e9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Build the best model based on parameter search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b41ace6c-a208-4e02-9632-4cfdf0b43076",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_params_localRF = {\n",
    "    \"class_weight\": \"balanced\",\n",
    "    \"criterion\": \"entropy\",\n",
    "    \"max_depth\": 15,\n",
    "    \"max_features\": \"log2\",\n",
    "    \"min_samples_leaf\": 2,\n",
    "    \"min_samples_split\": 4,\n",
    "    \"splitter\": \"best\",\n",
    "}\n",
    "\n",
    "\n",
    "# Build model based on the best hyperparameters found\n",
    "build_wrapper = lambda partition_iter: build_local_tree(\n",
    "    partition_iter, best_params_localRF\n",
    ")\n",
    "best_models_localRF = rdd_train.mapPartitions(build_wrapper).collect()\n",
    "\n",
    "# Predict\n",
    "transform_wrapper = lambda instance: transform(instance, best_models_localRF)\n",
    "df_pred = rdd_test.map(transform_wrapper).toDF()\n",
    "\n",
    "# Evaluate\n",
    "evaluation_best_localRF = evaluator_localRF.evaluate(df_pred)\n",
    "\n",
    "print(\n",
    "    f\"Evaluation of the best model with Local approach Random Forest: {evaluation_best_localRF:.6f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b1a6edc-eea9-4ee6-8ac2-b35b4661d323",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pred.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "164620d2-3ff4-4945-91b4-87d0fb9c95b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "predictionAndLabels_localRF = df_pred.select(\"raw_prediction\", \"outcome\")\n",
    "# Rename columns to fit MulticlassMetrics\n",
    "predictionAndLabels_localRF = predictionAndLabels_localRF.withColumnRenamed(\n",
    "    \"raw_prediction\", \"prediction\"\n",
    ").withColumnRenamed(\"outcome\", \"label\")\n",
    "\n",
    "eval_lrf = MulticlassMetrics(predictionAndLabels_localRF.rdd)\n",
    "\n",
    "accuracy_lrf = eval_lrf.accuracy\n",
    "precision_lrf = eval_lrf.precision(1.0)\n",
    "recall_lrf = eval_lrf.recall(1.0)\n",
    "f1_lrf = eval_lrf.fMeasure(1.0, 1.0)\n",
    "confusion_matrix_lrf = eval_lrf.confusionMatrix().toArray()\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy_lrf:.6f}\")\n",
    "print(f\"Precision: {precision_lrf:.6f}\")\n",
    "print(f\"Recall: {recall_lrf:.6f}\")\n",
    "print(f\"F1 Score: {f1_lrf:.6f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix_lrf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "602e6197-6a90-40ed-bcee-8652a4e4310b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Global Approach Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f2d1c13-d8e7-44b0-ba82-fc4c47a5113d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "def build_global_rf(\n",
    "    train_df: DataFrame, parameters: dict = None, param_grid: dict = None\n",
    ") -> RandomForestClassificationModel:\n",
    "    \"\"\"\n",
    "    Builds a global random forest classifier model using the given training data.\n",
    "\n",
    "    Optionally, hyperparameters can be provided for the random forest classifier,\n",
    "    or a grid of hyperparameters for cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "        train_df (Dataframe): The training data as a DataFrame.\n",
    "        parameters (dict, optional): Optional parameters for the random forest classifier. Defaults to None.\n",
    "        param_grid (list, optional): Optional grid of parameters for cross-validation. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        RandomForestClassifierModel: The trained random forest classifier model.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If both 'parameters' and 'param_grid' are provided.\n",
    "    \"\"\"\n",
    "\n",
    "    if (parameters is not None) and (param_grid is not None):\n",
    "\n",
    "        raise ValueError(\n",
    "            \"Only one of 'parameters' and 'param_grid' should be provided.\"\n",
    "        )\n",
    "    else:  # Pylance is complaining unreachable after value error, this fixes.\n",
    "        pass\n",
    "\n",
    "    if parameters is None:\n",
    "        parameters = {}\n",
    "\n",
    "    # Create a vector assembler to combine features into a single vector column\n",
    "    input_cols = [col for col in train_df.columns if col != \"outcome\"]\n",
    "    assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "\n",
    "    # Create a random forest classifier\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        labelCol=\"outcome\", featuresCol=\"features\", **parameters\n",
    "    )\n",
    "    if param_grid is not None:\n",
    "\n",
    "        built_param_grid = ParamGridBuilder()\n",
    "        for param_name, param_values in param_grid.items():\n",
    "            built_param_grid = built_param_grid.addGrid(\n",
    "                rf.getParam(param_name), param_values\n",
    "            )\n",
    "        built_param_grid = built_param_grid.build()\n",
    "\n",
    "        cv = CrossValidator(\n",
    "            estimator=rf,\n",
    "            estimatorParamMaps=built_param_grid,\n",
    "            evaluator=BinaryClassificationEvaluator(\n",
    "                labelCol=\"outcome\",\n",
    "                metricName=\"areaUnderROC\",\n",
    "            ),\n",
    "            numFolds=4,\n",
    "            seed=42,  # Seed for reproducibility, can be changed/removed\n",
    "        )\n",
    "        pipeline = Pipeline(stages=[assembler, cv])\n",
    "    else:\n",
    "\n",
    "        pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "    # Fit the pipeline on the train set\n",
    "    model = pipeline.fit(train_df)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24d34f28-f6a7-43e2-9bf9-aa9aa4cf8b84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"bootstrap\": [\n",
    "        True\n",
    "    ],  # Good for our dataset to stop overfitting, reduces tree correlation\n",
    "    \"featureSubsetStrategy\": [\n",
    "        \"auto\"\n",
    "    ],  # Number of features to consider for splitting at each node\n",
    "    \"subsamplingRate\": [\n",
    "        1.0\n",
    "    ],  # Fraction of training data used for learning each decision tree\n",
    "    \"impurity\": [\"gini\", \"entropy\"],\n",
    "    \"maxBins\": [16, 32, 64],\n",
    "    \"maxDepth\": [5, 10, 15],\n",
    "    \"minInfoGain\": [0.0, 0.1, 0.2],\n",
    "    \"minInstancesPerNode\": [1, 2, 4],\n",
    "    \"numTrees\": [10, 20, 30],\n",
    "}\n",
    "\n",
    "global_rf_model = build_global_rf(train_df, param_grid=param_grid)\n",
    "best_global_rf_model = global_rf_model.stages[1]\n",
    "best_global_rf_model = best_global_rf_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24c411a1-44ad-4615-ac0f-d9161f29f995",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_global_rf_params = {\n",
    "    \"bootstrap\": best_global_rf_model.getBootstrap(),\n",
    "    \"featureSubsetStrategy\": best_global_rf_model.getFeatureSubsetStrategy(),\n",
    "    \"subsamplingRate\": best_global_rf_model.getSubsamplingRate(),\n",
    "    \"impurity\": best_global_rf_model.getImpurity(),\n",
    "    \"maxBins\": best_global_rf_model.getMaxBins(),\n",
    "    \"maxDepth\": best_global_rf_model.getMaxDepth(),\n",
    "    \"minInfoGain\": best_global_rf_model.getMinInfoGain(),\n",
    "    \"minInstancesPerNode\": best_global_rf_model.getMinInstancesPerNode(),\n",
    "    \"numTrees\": best_global_rf_model.getNumTrees,\n",
    "}\n",
    "\n",
    "print(best_global_rf_params)\n",
    "# {\n",
    "#     \"bootstrap\": True,\n",
    "#     \"featureSubsetStrategy\": \"auto\",\n",
    "#     \"subsamplingRate\": 1.0,\n",
    "#     \"impurity\": \"gini\",\n",
    "#     \"maxBins\": 64,\n",
    "#     \"maxDepth\": 15,\n",
    "#     \"minInfoGain\": 0.0,\n",
    "#     \"minInstancesPerNode\": 1,\n",
    "#     \"numTrees\": 30,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87d848aa-864a-48b0-aee4-5f42661f1f31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_global_rf_params = {\n",
    "    \"bootstrap\": True,\n",
    "    \"featureSubsetStrategy\": \"auto\",\n",
    "    \"subsamplingRate\": 1.0,\n",
    "    \"impurity\": \"gini\",\n",
    "    \"maxBins\": 64,\n",
    "    \"maxDepth\": 15,\n",
    "    \"minInfoGain\": 0.0,\n",
    "    \"minInstancesPerNode\": 1,\n",
    "    \"numTrees\": 30,\n",
    "}\n",
    "global_rf = build_global_rf(train_df, parameters=best_global_rf_params)\n",
    "# global_rf = global_rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d5f7034-c591-445d-b479-497bb078301b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions_global_rf = global_rf.transform(test_df)\n",
    "evaluator_global_rf = BinaryClassificationEvaluator(\n",
    "    labelCol=\"outcome\", metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "evaluation_best_global_rf = evaluator_global_rf.evaluate(predictions_global_rf)\n",
    "print(\n",
    "    f\"Evaluation of the best model with Global approach Random Forest: {evaluation_best_global_rf:.6f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df1b5f15-d3d9-4556-b4ba-b416984dbec3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_grf = MulticlassMetrics(predictions_global_rf.select(\"prediction\", \"outcome\").rdd)\n",
    "\n",
    "accuracy_grf = eval_grf.accuracy\n",
    "precision_grf = eval_grf.precision(1.0)\n",
    "recall_grf = eval_grf.recall(1.0)\n",
    "f1_grf = eval_grf.fMeasure(1.0, 1.0)\n",
    "confusion_matrix_grf = eval_grf.confusionMatrix().toArray()\n",
    "\n",
    "print(f\"Accuracy: {accuracy_grf:.6f}\")\n",
    "print(f\"Precision: {precision_grf:.6f}\")\n",
    "print(f\"Recall: {recall_grf:.6f}\")\n",
    "print(f\"F1 Score: {f1_grf:.6f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix_grf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e5157a1-b671-49d6-a7b3-e346cd928631",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Construct metrics dataframe\n",
    "metrics = pd.DataFrame(\n",
    "    [\n",
    "        [evaluation_best_localRF, evaluation_best_global_rf],\n",
    "        [accuracy_lrf, accuracy_grf],\n",
    "        [precision_lrf, precision_grf],\n",
    "        [recall_lrf, recall_grf],\n",
    "        [f1_lrf, f1_grf],\n",
    "    ],\n",
    "    index=[\"Area Under ROC\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"],\n",
    "    columns=[\"Local RF\", \"Global RF\"],\n",
    ")\n",
    "\n",
    "display(metrics)\n",
    "\n",
    "# \tLocal RF\tGlobal RF\n",
    "# Area Under ROC\t0.804701\t0.932821\n",
    "# Accuracy\t0.964200\t0.964200\n",
    "# Precision\t0.388889\t0.357143\n",
    "# Recall\t0.636364\t0.454545\n",
    "# F1\t0.482759\t0.400000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8de54dc-451b-4cd1-9451-8d689949df3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Increase font size\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix, title, axis=None):\n",
    "    \"\"\"\n",
    "    Plot the confusion matrix as a heatmap.\n",
    "\n",
    "    Parameters:\n",
    "    confusion_matrix : np.ndarray\n",
    "        The confusion matrix to plot.\n",
    "\n",
    "    title : str\n",
    "        The title of the plot.\n",
    "\n",
    "    axis : matplotlib.axes.Axes, optional\n",
    "        The axis on which to plot the confusion matrix. If not provided, a new figure will be created.\n",
    "    \"\"\"\n",
    "    if axis is None:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        axis = plt.gca()\n",
    "\n",
    "    sns.heatmap(\n",
    "        confusion_matrix,\n",
    "        annot=True,\n",
    "        fmt=\"g\",\n",
    "        cmap=\"Blues\",\n",
    "        cbar=False,\n",
    "        annot_kws={\"size\": 15},\n",
    "        ax=axis,\n",
    "    )\n",
    "    axis.set_xlabel(\"Predicted\")\n",
    "    axis.set_ylabel(\"Actual\")\n",
    "    # axis.set_title(title)\n",
    "\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix_lrf, \"Local RF Confusion Matrix\")\n",
    "plt.savefig(\"Local_RF_Confusion_Matrix_notitle.pdf\")\n",
    "plot_confusion_matrix(confusion_matrix_grf, \"Global RF Confusion Matrix\")\n",
    "plt.savefig(\"Global_RF_Confusion_Matrix_notitle.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f82e42d-e4b9-4b87-b580-8311bdd3a54f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "times_train_and_predict_global_rf = time_analysis(\n",
    "    train_df,\n",
    "    lambda train_df: build_global_rf(\n",
    "        train_df, parameters=best_global_rf_params\n",
    "    ).transform(test_df),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2cb515e-7773-4e9a-81f2-29b1483e2600",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sns.reset_orig()\n",
    "plot_time_analysis(\n",
    "    times_train_and_predict_global_rf,\n",
    "    \"Time Analysis of Training and Prediction with Global approach Random Forest\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ML_Pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "bdenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
