{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a03d9257-9bc7-4233-bd56-59dcb611a339",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<h1>Feature Creation and Data Preprocessing Pipeline</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83382c86-aa86-4569-95de-313385076401",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tkvX_hiE5G8s",
    "outputId": "a7f3903a-cdb5-40d8-e032-c736f7fc6282"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .appName(\"FeatureCreation\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51fa0709-74cb-447d-8793-3f2e05124384",
     "showTitle": false,
     "title": ""
    },
    "id": "yEZWpxi1qIgy"
   },
   "outputs": [],
   "source": [
    "# Import files\n",
    "df_bids = spark.read.csv(\"bids.csv\", header=True, inferSchema=True).cache()\n",
    "df_train = spark.read.csv(\"train.csv\", header=True, inferSchema=True).cache()\n",
    "\n",
    "# check types\n",
    "if DEBUG:\n",
    "    df_bids.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3496178d-42d4-46ed-9ba3-1261bb99bf34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ip_flag(df_bids: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    This function takes a DataFrame of bids and returns a DataFrame with an IP flag indicating whether a bidder shares their IP address with other bidders.\n",
    "\n",
    "    Parameters:\n",
    "        df_bids (DataFrame): The input DataFrame containing bidder_id and ip columns.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with bidder_id, ip, and shared_ip columns. The shared_ip column indicates whether the bidder shares their IP address with other bidders (True or False).\n",
    "    \"\"\"\n",
    "    uniqe_users = df_bids.select(\"bidder_id\", \"ip\").distinct()\n",
    "    # uniqe_users.show()\n",
    "    # print(uniqe_users.count())\n",
    "\n",
    "    # group by IP, collect users Ids, we neeed to group to get rid of repeating IPs\n",
    "    uniqe_ips = uniqe_users.groupBy(\"ip\").agg(\n",
    "        collect_set(\"bidder_id\").alias(\"bidder_ids\")\n",
    "    )\n",
    "\n",
    "    uniqe_ips = uniqe_ips.withColumn(\"shared_ip\", size(uniqe_ips.bidder_ids) > 1)\n",
    "    # uniqe_ips.show()\n",
    "\n",
    "    # Join back with the original DataFrame to get the boolean value for each user\n",
    "    result = (\n",
    "        uniqe_users.join(uniqe_ips, \"ip\", \"left_outer\")\n",
    "        .select(\"bidder_id\", \"ip\", \"shared_ip\")\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    # Show the result\n",
    "    # tmp=result.groupBy('ip').agg(collect_set(\"bidder_id\").alias(\"bidder_ids\"))\n",
    "    # tmp.show()\n",
    "    # print(tmp.filter(tmp.ip==\"0.101.161.187\").collect())\n",
    "\n",
    "    # result.printSchema()\n",
    "\n",
    "    result = (\n",
    "        result.groupBy(\"bidder_id\")\n",
    "        .agg(max(result.shared_ip))\n",
    "        .withColumnRenamed(\"max(shared_ip)\", \"shared_ip\")\n",
    "    )  # True is bigger than False, so if any is true it will result in True\n",
    "    # result.filter(result.bidder_id=='37bf6f23b628a3e2b5b22ba81beccbef0efoh').show()\n",
    "    # result.filter(result.bidder_id=='ffbc0fdfbf19a8a9116b68714138f2902cc13').show()\n",
    "    # result.show()\n",
    "\n",
    "    # print(result.groupBy(\"max(shared_ip)\").sum().show())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f95a3e7a-9b1a-4d51-b0ab-2f79d0e4ef12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convert_time(df_bids: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the 'time' column in the DataFrame to timestamp format and adds a new 'day' column with the date.\n",
    "\n",
    "    Parameters:\n",
    "        df_bids (DataFrame): The input DataFrame containing the 'time' column.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The updated DataFrame with the 'time' column converted to timestamp format and a new 'day' column added.\n",
    "    \"\"\"\n",
    "    df_bids = df_bids.withColumn(\"time\", to_timestamp(col(\"time\")))\n",
    "    df_bids = df_bids.withColumn(\"day\", to_date(\"time\"))\n",
    "    return df_bids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdbae1b3-412d-49bd-8abb-3ebdf0363419",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def replace_nan_countries(df_bids: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Replace missing values in the 'country' column of the DataFrame with 'unk_ctry'.\n",
    "\n",
    "    Parameters:\n",
    "        df_bids (DataFrame): The input DataFrame containing the 'country' column.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with missing values in the 'country' column replaced.\n",
    "\n",
    "    \"\"\"\n",
    "    if DEBUG:\n",
    "        df_bids.select(\n",
    "            [count(when(isnull(c), c)).alias(c) for c in df_bids.columns]\n",
    "        ).show()\n",
    "\n",
    "    df_bids: DataFrame = df_bids.fillna(value=\"unk_ctry\", subset=[\"country\"])\n",
    "\n",
    "    if DEBUG:\n",
    "        df_bids.select(\n",
    "            [count(when(isnull(c), c)).alias(c) for c in df_bids.columns]\n",
    "        ).show()\n",
    "\n",
    "    return df_bids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69cd8aee-edae-43ec-819f-64661ec71da5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def bids_per_auction(df_bids: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the number of bids per auction for each bidder.\n",
    "\n",
    "    Parameters:\n",
    "        df_bids (DataFrame): The input DataFrame containing the bids data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with the number of bids per auction for each bidder.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df_bids.groupBy(\"bidder_id\", \"auction\")\n",
    "        .count()\n",
    "        .withColumnRenamed(\"count\", \"bids_count\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f04f3817-646f-4bd2-b4a7-e21c31809d3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def mean_bids_per_auction(df_bids: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the mean number of bids per auction for each bidder.\n",
    "\n",
    "    Parameters:\n",
    "        df_bids (DataFrame): The input DataFrame containing the bids data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with the mean number of bids per auction for each bidder.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        bids_per_auction(df_bids)\n",
    "        .groupBy(\"bidder_id\")\n",
    "        .avg(\"bids_count\")\n",
    "        .withColumnRenamed(\"avg(bids_count)\", \"mean_bids_per_auction\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3320db2-6da8-4383-b400-39421084ec84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def unique_devices_per_bidder(df_bids: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the number of unique devices used by each bidder.\n",
    "\n",
    "    Parameters:\n",
    "        df_bids (DataFrame): The DataFrame containing the bids data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with the number of unique devices per bidder.\n",
    "    \"\"\"\n",
    "    return df_bids.groupBy(\"bidder_id\").agg(\n",
    "        countDistinct(\"device\").alias(\"unique_devices\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65f87076-0606-44c2-8c22-dc3b30167e2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def unique_ips_per_bidder(df_bids: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the number of unique IP addresses per bidder.\n",
    "\n",
    "    Parameters:\n",
    "        df_bids (DataFrame): The DataFrame containing the bids data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with the number of unique IP addresses per bidder.\n",
    "    \"\"\"\n",
    "    return df_bids.groupBy(\"bidder_id\").agg(countDistinct(\"ip\").alias(\"unique_ips\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd72bcc2-8ef0-429e-98d8-8f924f97fd5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def unique_countries_per_bidder(df_bids: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the number of unique countries per bidder in the given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df_bids (DataFrame): The DataFrame containing the bids data.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A new DataFrame with the number of unique countries per bidder.\n",
    "    \"\"\"\n",
    "    return df_bids.groupBy(\"bidder_id\").agg(\n",
    "        countDistinct(\"country\").alias(\"unique_countries\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "219808e7-cbe7-4c4b-a5dd-6ddff1fe7a01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def auction_frequency(df_bids: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the frequency of auctions for each bidder.\n",
    "\n",
    "    Parameters:\n",
    "        df_bids (DataFrame): The DataFrame containing the bids data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with the bidder ID and the corresponding auction frequency.\n",
    "    \"\"\"\n",
    "    return df_bids.groupBy(\"bidder_id\").agg(\n",
    "        countDistinct(\"auction\").alias(\"auction_frequency\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fff447ad-e92a-467b-8fd9-a1810b2f7f13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def unique_url_per_bidder(df_bids: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the number of unique URLs per bidder in the given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df_bids (DataFrame): The DataFrame containing the bids data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with the number of unique URLs per bidder.\n",
    "\n",
    "    \"\"\"\n",
    "    return df_bids.groupBy(\"bidder_id\").agg(countDistinct(\"url\").alias(\"unique_url\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25a235ef-ca0b-4b73-ab5e-d3fa3727bf40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def unique_merchandise_per_bidder(df_bids: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the number of unique merchandise per bidder.\n",
    "\n",
    "    Parameters:\n",
    "        df_bids (DataFrame): The input DataFrame containing bid data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with the count of unique merchandise per bidder.\n",
    "    \"\"\"\n",
    "    return df_bids.groupBy(\"bidder_id\").agg(\n",
    "        countDistinct(\"merchandise\").alias(\"unique_merchandise\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b1e23e0-9b9c-4514-8d27-5eb87b99bed2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main_merchandise(df_bids: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Assigns the main merchandise for each bidder based on the merchandise with the most bids.\n",
    "\n",
    "    Parameters:\n",
    "        df_bids (DataFrame): DataFrame containing the bids data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with the main merchandise for each bidder.\n",
    "    \"\"\"\n",
    "\n",
    "    main_merchandise = df_bids.groupBy(\"bidder_id\", \"merchandise\").count()\n",
    "    max_merchandise = (\n",
    "        main_merchandise.groupby(\"bidder_id\")\n",
    "        .agg({\"count\": \"max\"})\n",
    "        .withColumnRenamed(\"max(count)\", \"count\")\n",
    "    )\n",
    "    main_merchandise = main_merchandise.join(\n",
    "        max_merchandise, [\"bidder_id\", \"count\"]\n",
    "    ).select(\"bidder_id\", \"merchandise\")\n",
    "\n",
    "    # CACHE - IMPORTANT\n",
    "    main_merchandise.cache()\n",
    "\n",
    "    # if multiple merchandises per bidder\n",
    "    if (\n",
    "        main_merchandise.count()\n",
    "        != main_merchandise.select(\"bidder_id\").distinct().count()\n",
    "    ):\n",
    "        main_merchandise = main_merchandise.groupBy(\"bidder_id\").agg(\n",
    "            collect_set(\"merchandise\").alias(\"merchandises\")\n",
    "        )\n",
    "        main_merchandise = main_merchandise.withColumn(\n",
    "            \"mul_merch\", size(main_merchandise.countries) > 1\n",
    "        )\n",
    "        main_merchandise = main_merchandise.withColumn(\n",
    "            \"merchandise\",\n",
    "            when(main_merchandise.mul_merch, \"multiple\").otherwise(\n",
    "                main_merchandise.merchandises[0]\n",
    "            ),\n",
    "        )\n",
    "        main_merchandise = main_merchandise.drop(\"merchandises\", \"mul_merch\")\n",
    "\n",
    "    main_merchandise = main_merchandise.withColumnRenamed(\n",
    "        \"merchandise\", \"top_merchandise\"\n",
    "    )\n",
    "    return main_merchandise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f5780a2-21ec-48dd-9081-d50f9e7f7118",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main_country(df_bids: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts the main country for each bidder based on the number of bids made in each country.\n",
    "\n",
    "    Parameters:\n",
    "        df_bids (DataFrame): The input DataFrame containing bidder information.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with the main country for each bidder.\n",
    "    \"\"\"\n",
    "\n",
    "    # Getting main country - multiple if more than one main country\n",
    "\n",
    "    main_country = df_bids.groupBy(\"bidder_id\", \"country\").count()\n",
    "    max_country = (\n",
    "        main_country.groupby(\"bidder_id\")\n",
    "        .agg({\"count\": \"max\"})\n",
    "        .withColumnRenamed(\"max(count)\", \"count\")\n",
    "    )\n",
    "\n",
    "    main_country = main_country.join(max_country, [\"bidder_id\", \"count\"]).select(\n",
    "        \"bidder_id\", \"country\"\n",
    "    )\n",
    "\n",
    "    # CACHE - IMPORTANT\n",
    "    main_country.cache()\n",
    "\n",
    "    if main_country.count() != main_country.select(\"bidder_id\").distinct().count():\n",
    "        main_country = main_country.groupBy(\"bidder_id\").agg(\n",
    "            collect_set(\"country\").alias(\"countries\")\n",
    "        )\n",
    "        main_country = main_country.withColumn(\n",
    "            \"mul_ctry\", size(main_country.countries) > 1\n",
    "        )\n",
    "        main_country = main_country.withColumn(\n",
    "            \"country\",\n",
    "            when(main_country.mul_ctry, \"multiple\").otherwise(\n",
    "                main_country.countries[0]\n",
    "            ),\n",
    "        )\n",
    "        main_country = main_country.drop(\"countries\", \"mul_ctry\")\n",
    "\n",
    "    main_country = main_country.withColumnRenamed(\"country\", \"top_country\")\n",
    "    return main_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79597aa4-2de4-410e-af13-9b80b7a10a87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def median_time_diff_per_bidder(df_bids: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the median time difference between bids for each bidder.\n",
    "\n",
    "    Parameters:\n",
    "        df_bids (DataFrame): DataFrame containing the bids data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with the median time difference per bidder.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Median time difference per bid, if only one bid per user - median of the rest of the results\n",
    "    from pyspark.sql.functions import col, to_timestamp, lag, expr\n",
    "    from pyspark.sql import Window\n",
    "    from pyspark.ml.feature import Imputer\n",
    "\n",
    "    # Define window specification to calculate lag per bidder ordered by time\n",
    "    windowSpec = Window.partitionBy(\"bidder_id\").orderBy(\"time\")\n",
    "\n",
    "    # Calculate previous time and time difference\n",
    "    df_bids_sorted = df_bids.withColumn(\"prev_time\", lag(\"time\", 1).over(windowSpec))\n",
    "    df_bids_sorted = df_bids_sorted.withColumn(\n",
    "        \"time_diff\", (col(\"time\").cast(\"long\") - col(\"prev_time\").cast(\"long\"))\n",
    "    )\n",
    "\n",
    "    # Aggregate to find median time difference per bidder\n",
    "    median_time_diff_per_bidder = df_bids_sorted.groupBy(\"bidder_id\").agg(\n",
    "        expr(\"percentile_approx(time_diff, 0.5)\").alias(\"median_time_diff\")\n",
    "    )\n",
    "\n",
    "    # Initialize the Imputer to fill missing values in 'median_time_diff'\n",
    "    imputer = Imputer(\n",
    "        inputCols=[\"median_time_diff\"],  # Correct column name for imputation\n",
    "        outputCols=[\"median_time_diff\"],\n",
    "    ).setStrategy(\"median\")\n",
    "\n",
    "    # Apply the imputer to the DataFrame containing median time differences\n",
    "    median_time_diff_per_bidder = imputer.fit(median_time_diff_per_bidder).transform(\n",
    "        median_time_diff_per_bidder\n",
    "    )\n",
    "\n",
    "    return median_time_diff_per_bidder\n",
    "\n",
    "    # Join the imputed median time differences to the features dataset\n",
    "    # features_df = features_df.join(median_time_diff_per_bidder, 'bidder_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58b8251f-2c40-4e97-aee4-2c30a34dda32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def bidder_entropy(df_bids: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the entropy (randomness) of the bidder's actions per day.\n",
    "    A higher number indicates higher randomness, more human-like.\n",
    "\n",
    "    Parameters:\n",
    "        df_bids (DataFrame): The DataFrame containing the bids data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with the entropy per bidder.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, log2\n",
    "    from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "    # Calculate bid counts per day per bidder\n",
    "    # df_bids.show()\n",
    "\n",
    "    bids_per_day = df_bids.groupBy(\"bidder_id\", \"day\").count()\n",
    "\n",
    "    # Calculate total bids per bidder\n",
    "    total_bids = bids_per_day.groupBy(\"bidder_id\").agg(\n",
    "        spark_sum(\"count\").alias(\"total_bids\")\n",
    "    )\n",
    "\n",
    "    # Join to get total bids alongside each day's count\n",
    "    bids_per_day = bids_per_day.join(total_bids, \"bidder_id\")\n",
    "\n",
    "    # Calculate probability of each day's bids\n",
    "    bids_per_day = bids_per_day.withColumn(\n",
    "        \"probability\", col(\"count\") / col(\"total_bids\")\n",
    "    )\n",
    "\n",
    "    # Calculate entropy components\n",
    "    bids_per_day = bids_per_day.withColumn(\n",
    "        \"entropy_component\", -col(\"probability\") * log2(col(\"probability\"))\n",
    "    )\n",
    "\n",
    "    # Aggregate entropy per bidder\n",
    "    bidder_entropy = bids_per_day.groupBy(\"bidder_id\").agg(\n",
    "        spark_sum(\"entropy_component\").alias(\"day_entropy\")\n",
    "    )\n",
    "\n",
    "    return bidder_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea6808bf-a645-487a-8aa6-e825c836bba7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def count_nulls(df_to_count: DataFrame):\n",
    "    \"\"\"\n",
    "    Counts the number of null values in each column of the given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df_to_count (DataFrame): The DataFrame to count null values from.\n",
    "\n",
    "    Returns:\n",
    "        None, prints the number of null values in each column of the DataFrame.\n",
    "    \"\"\"\n",
    "    null_counts = [\n",
    "        sum(col(column).isNull().cast(\"int\")).alias(column)\n",
    "        for column in df_to_count.columns\n",
    "    ]\n",
    "    df_null_counts = df_to_count.agg(*null_counts)\n",
    "    df_null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "356bddb9-bd5d-45d9-b1ed-1c0011d07872",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def remove_strings(df_features: DataFrame):\n",
    "    \"\"\"\n",
    "    Removes string columns from the DataFrame and performs label encoding on specific columns.\n",
    "\n",
    "    Parameters:\n",
    "        df_features (DataFrame): The input DataFrame containing string columns.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The modified DataFrame with string columns removed and label encoding applied.\n",
    "    \"\"\"\n",
    "    from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "    # Label Encoding\n",
    "    indexer = StringIndexer(inputCol=\"top_country\", outputCol=\"top_country_index\")\n",
    "    df_features = indexer.fit(df_features).transform(df_features).drop(\"top_country\")\n",
    "\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=\"top_merchandise\", outputCol=\"top_merchandise_index\"\n",
    "    )\n",
    "    df_features = (\n",
    "        indexer.fit(df_features).transform(df_features).drop(\"top_merchandise\")\n",
    "    )\n",
    "\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16c8e881-693e-4fd8-abb5-a7c97f480114",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def move_outcome(df_features: DataFrame):\n",
    "    \"\"\"\n",
    "    Moves the 'outcome' column to the end of the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df_features (DataFrame): The input DataFrame containing the features.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The modified DataFrame with the 'outcome' column moved to the end.\n",
    "    \"\"\"\n",
    "    # move outcome to the end\n",
    "    columns = df_features.columns\n",
    "    order = [col_name for col_name in columns if col_name != \"outcome\"] + [\"outcome\"]\n",
    "    df_features = df_features.select(*order)\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7449d84c-6128-4f07-bf4c-c28264ba8a95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_features(\n",
    "    df_bidders: DataFrame, df_bids: DataFrame, keep_strings=False\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Create final features for bidders based on the given dataframes.\n",
    "\n",
    "    Parameters:\n",
    "        df_bidders (DataFrame): The dataframe containing bidder information.\n",
    "        df_bids (DataFrame): The dataframe containing bid information.\n",
    "        keep_strings (bool, optional): Flag to indicate whether to keep string features. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The dataframe with the created features.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # clear bids first so no time is wasted calculating bidders we are not interested in\n",
    "\n",
    "    # pre-process\n",
    "    df_bids = convert_time(replace_nan_countries(df_bids)).cache()\n",
    "    df_bids_main = (\n",
    "        df_bidders.join(df_bids, \"bidder_id\", \"inner\")\n",
    "        .select(df_bids[\"*\"], df_bidders[\"outcome\"])\n",
    "        .cache()\n",
    "    )\n",
    "    df_bids_base = df_bidders.select(\"bidder_id\", \"outcome\")\n",
    "\n",
    "    features_df = (\n",
    "        df_bids_base.join(mean_bids_per_auction(df_bids_main), \"bidder_id\")\n",
    "        .join(unique_devices_per_bidder(df_bids_main), \"bidder_id\")\n",
    "        .join(unique_ips_per_bidder(df_bids_main), \"bidder_id\")\n",
    "        .join(unique_countries_per_bidder(df_bids_main), \"bidder_id\")\n",
    "        .join(auction_frequency(df_bids_main), \"bidder_id\")\n",
    "        .join(unique_url_per_bidder(df_bids_main), \"bidder_id\")\n",
    "        .join(unique_merchandise_per_bidder(df_bids_main), \"bidder_id\")\n",
    "        .join(main_merchandise(df_bids_main), \"bidder_id\")\n",
    "        .join(main_country(df_bids_main), \"bidder_id\")\n",
    "        .join(median_time_diff_per_bidder(df_bids_main), \"bidder_id\")\n",
    "        .join(bidder_entropy(df_bids), \"bidder_id\")\n",
    "        .join(ip_flag(df_bids), \"bidder_id\")\n",
    "    )\n",
    "\n",
    "    if keep_strings:\n",
    "        return move_outcome(features_df)\n",
    "    else:\n",
    "        return move_outcome(remove_strings(features_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f38cdc95-55a5-44df-98c4-2b25a0c02a64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ft = create_features(df_train, df_bids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e71e93a9-71e1-42c3-8eaf-20f4bf1a50c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ft.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f245719-35cd-403b-bd65-f8d9e43f27c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "count_nulls(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78d47d0d-31c4-4b60-8c17-2c2aae94503c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(ft.select(\"bidder_id\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd3741d-b66a-46ac-844d-badddd365165",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save features csv\n",
    "ft.toPandas().to_csv(\"features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0f2895c-c7df-4db0-b2ee-b3c655c98150",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### TIME ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98b2e318-6bac-4311-93ff-45f6bc15f436",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "def time_analysis(df: DataFrame, func) -> dict:\n",
    "    \"\"\"\n",
    "    Apply the provided function to different fractions (subsets) of the DataFrame and measure the time taken\n",
    "    for each function run.\n",
    "\n",
    "    Parameters:\n",
    "    df : DataFrame\n",
    "        The DataFrame to apply the function to.\n",
    "    func : function\n",
    "        The function to apply to the DataFrame. The function should take a DataFrame as its only argument.\n",
    "\n",
    "    Returns:\n",
    "    dict\n",
    "        A dictionary containing the fraction of DataFrame and the corresponding time taken for the function to run.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    df_length = df.count()\n",
    "\n",
    "    for percentage in range(10, 110, 10):\n",
    "        # Calculate the number of rows for the given percentage\n",
    "        num_rows = int(df_length * (percentage / 100))\n",
    "        partial_df = df.limit(num_rows).cache()\n",
    "\n",
    "        # Measure the time taken to apply the function\n",
    "        start_time = time()\n",
    "        func(partial_df)\n",
    "        end_time = time()\n",
    "\n",
    "        # Store the results\n",
    "        results[float(percentage / 100)] = end_time - start_time\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e47b6a6f-be21-4f53-96f2-e7d592a0b0d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "\n",
    "def plot_time_analysis(times_partitioning: dict, plot_title: str):\n",
    "    \"\"\"\n",
    "    Plot the time taken for the function to run for different fractions of the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    times_partitioning : dict\n",
    "        A dictionary containing the fraction of DataFrame and the corresponding time taken for the function to run.\n",
    "    plot_title : str\n",
    "        The title of the plot.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.plot(list(times_partitioning.keys()), list(times_partitioning.values()))\n",
    "\n",
    "    plt.xlabel(\"Fraction of DataFrame\")\n",
    "\n",
    "    plt.ylabel(\"Time taken (s)\")\n",
    "    # plt.title(plot_title)\n",
    "\n",
    "    # Change the plot title to have only file supported characters(no spaces)\n",
    "    plot_filename = \"\".join(e for e in plot_title if e.isalnum() or e == \"_\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(plot_filename + \".pdf\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dff85184-e08d-445e-b37e-c3180d4e3bb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def wrapper(df_bids):\n",
    "    ft = create_features(df_train, df_bids)\n",
    "    ft.collect()  # could be replaced with write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9171747c-df73-4aad-9ac1-80922e96cdd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "times = time_analysis(df_bids, wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89246d63-9707-4b17-8285-3325cb560c7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_time_analysis(times, \"Feature Creation Time Analysis\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Feature_Creation",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
